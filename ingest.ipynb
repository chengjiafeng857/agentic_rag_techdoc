{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201e2dc2",
   "metadata": {},
   "source": [
    "# Ingesting MDX and Jupyter Notebooks from documents/docs\n",
    "\n",
    "This notebook demonstrates a complete workflow for loading and processing all MDX and Jupyter notebook files from the `documents/docs` folder. We'll process these documents and store them in a vector database for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c12917a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbformat\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat)\n",
      "  Downloading fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./.venv/lib/python3.12/site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.12/site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in ./.venv/lib/python3.12/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.23.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in ./.venv/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.12.2)\n",
      "Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Downloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: fastjsonschema, nbformat\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [nbformat]\n",
      "\u001b[1A\u001b[2KSuccessfully installed fastjsonschema-2.21.1 nbformat-5.10.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ab1af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import glob\n",
    "import ssl\n",
    "import urllib3\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import nbformat\n",
    "\n",
    "# Import langchain components\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders.notebook import NotebookLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Import supabase\n",
    "from supabase.client import Client, create_client\n",
    "\n",
    "# Configure SSL context to be more permissive to handle SSL connection issues\n",
    "ssl_context = ssl.create_default_context()\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f0df2",
   "metadata": {},
   "source": [
    "## Configure Database and Embedding Model\n",
    "\n",
    "Set up connections to Supabase and initialize the OpenAI embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476330ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supabase connection established: True\n",
      "Embeddings model initialized: True\n"
     ]
    }
   ],
   "source": [
    "# Initialize Supabase client with improved connection settings\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "supabase: Client = create_client(\n",
    "supabase_url, \n",
    "supabase_key,\n",
    "    options={\n",
    "        \"timeout\": 60,  # Increase timeout\n",
    "        \"headers\": {\n",
    "            \"Connection\": \"keep-alive\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize embeddings model with improved timeout and retry settings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "model=\"text-embedding-3-small\",\n",
    "    timeout=60,  # Increase timeout for API calls\n",
    "    max_retries=5  # Add retries for resilience\n",
    ")\n",
    "\n",
    "print(f\"Supabase connection established: {bool(supabase)}\")\n",
    "print(f\"Embeddings model initialized: {bool(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf7adb",
   "metadata": {},
   "source": [
    "## Define Custom MDX Loader\n",
    "\n",
    "Create a custom loader for MDX files that can extract content and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bbbd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDXLoader:\n",
    "    \"\"\"Loader for MDX files to extract content and frontmatter metadata.\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"Initialize with file path.\"\"\"\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load and parse MDX file.\"\"\"\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Extract metadata (if present at the beginning of the file)\n",
    "        metadata = {}\n",
    "        if content.startswith('---'):\n",
    "            # Extract the frontmatter between the first two '---' delimiters\n",
    "            parts = content.split('---', 2)\n",
    "            if len(parts) >= 3:\n",
    "                frontmatter = parts[1].strip()\n",
    "                # Simple parsing of key-value pairs\n",
    "                for line in frontmatter.split('\\n'):\n",
    "                    if ':' in line:\n",
    "                        key, value = line.split(':', 1)\n",
    "                        metadata[key.strip()] = value.strip()\n",
    "                content = parts[2]\n",
    "        \n",
    "        # Add file metadata\n",
    "        metadata['source'] = self.file_path\n",
    "        metadata['filetype'] = 'mdx'\n",
    "        \n",
    "        return [Document(page_content=content, metadata=metadata)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444045a",
   "metadata": {},
   "source": [
    "## Find All MDX and Jupyter Notebook Files\n",
    "\n",
    "Recursively locate all MDX and Jupyter notebook files in the docs directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc672fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1514 total files\n",
      "MDX files: 413\n",
      "Jupyter notebooks: 1101\n",
      "\n",
      "Sample MDX files:\n",
      "  - documents/docs/people.mdx\n",
      "  - documents/docs/introduction.mdx\n",
      "  - documents/docs/_templates/integration.mdx\n",
      "\n",
      "Sample Jupyter notebooks:\n",
      "  - documents/docs/versions/migrating_chains/stuff_docs_chain.ipynb\n",
      "  - documents/docs/versions/migrating_chains/refine_docs_chain.ipynb\n",
      "  - documents/docs/versions/migrating_chains/multi_prompt_chain.ipynb\n"
     ]
    }
   ],
   "source": [
    "def find_files(base_path, extensions):\n",
    "    \"\"\"Find all files with the given extensions in the base path recursively.\"\"\"\n",
    "    all_files = []\n",
    "    for ext in extensions:\n",
    "        # Use glob pattern to find all files with the extension recursively\n",
    "        pattern = os.path.join(base_path, '**', f'*.{ext}')\n",
    "        files = glob.glob(pattern, recursive=True)\n",
    "        all_files.extend(files)\n",
    "    return all_files\n",
    "\n",
    "# Define the base path and extensions\n",
    "base_path = 'documents/docs'\n",
    "extensions = ['mdx', 'ipynb']\n",
    "\n",
    "# Find all files\n",
    "files = find_files(base_path, extensions)\n",
    "print(f\"Found {len(files)} total files\")\n",
    "\n",
    "# Count files by type\n",
    "mdx_files = [f for f in files if f.endswith('.mdx')]\n",
    "ipynb_files = [f for f in files if f.endswith('.ipynb')]\n",
    "print(f\"MDX files: {len(mdx_files)}\")\n",
    "print(f\"Jupyter notebooks: {len(ipynb_files)}\")\n",
    "\n",
    "# Display sample files of each type\n",
    "if mdx_files:\n",
    "    print(f\"\\nSample MDX files:\")\n",
    "    for f in mdx_files[:3]:\n",
    "        print(f\"  - {f}\")\n",
    "if ipynb_files:\n",
    "    print(f\"\\nSample Jupyter notebooks:\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822dc16a",
   "metadata": {},
   "source": [
    "## Load and Process Documents\n",
    "\n",
    "Load all documents using the appropriate loader for each file type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df06a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Processing file 1/1514\n",
      "Processing file 51/1514\n",
      "Processing file 101/1514\n",
      "Processing file 151/1514\n",
      "Processing file 201/1514\n",
      "Processing file 251/1514\n",
      "Processing file 301/1514\n",
      "Processing file 351/1514\n",
      "Processing file 401/1514\n",
      "Processing file 451/1514\n",
      "Processing file 501/1514\n",
      "Processing file 551/1514\n",
      "Processing file 601/1514\n",
      "Processing file 651/1514\n",
      "Processing file 701/1514\n",
      "Processing file 751/1514\n",
      "Processing file 801/1514\n",
      "Processing file 851/1514\n",
      "Processing file 901/1514\n",
      "Processing file 951/1514\n",
      "Processing file 1001/1514\n",
      "Processing file 1051/1514\n",
      "Processing file 1101/1514\n",
      "Processing file 1151/1514\n",
      "Processing file 1201/1514\n",
      "Processing file 1251/1514\n",
      "Processing file 1301/1514\n",
      "Processing file 1351/1514\n",
      "Processing file 1401/1514\n",
      "Processing file 1451/1514\n",
      "Processing file 1501/1514\n",
      "Loaded 1514 documents\n"
     ]
    }
   ],
   "source": [
    "def load_documents(files):\n",
    "    \"\"\"Load all documents from the given file paths.\"\"\"\n",
    "    documents = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for i, file_path in enumerate(files):\n",
    "        try:\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Processing file {i+1}/{len(files)}\")\n",
    "                \n",
    "            if file_path.endswith('.mdx'):\n",
    "                loader = MDXLoader(file_path)\n",
    "                docs = loader.load()\n",
    "                documents.extend(docs)\n",
    "            elif file_path.endswith('.ipynb'):\n",
    "                loader = NotebookLoader(file_path, include_outputs=True, max_output_length=50)\n",
    "                docs = loader.load()\n",
    "                documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            failed_files.append((file_path, str(e)))\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    return documents, failed_files\n",
    "\n",
    "# Load all documents\n",
    "print(\"Loading documents...\")\n",
    "documents, failed_files = load_documents(files)\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"Failed to load {len(failed_files)} files\")\n",
    "    print(\"First few failures:\")\n",
    "    for path, error in failed_files[:3]:\n",
    "        print(f\"  - {path}: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee52fe",
   "metadata": {},
   "source": [
    "## Analyze Document Metadata\n",
    "\n",
    "Examine the metadata of the loaded documents to understand what we've collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a8759e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types:\n",
      "  - mdx: 413\n",
      "  - unknown: 1101\n",
      "\n",
      "Metadata keys found:\n",
      "  - description\n",
      "  - filetype\n",
      "  - hide_table_of_contents\n",
      "  - keywords\n",
      "  - pagination_next\n",
      "  - pagination_prev\n",
      "  - sidebar-position\n",
      "  - sidebar_class_name\n",
      "  - sidebar_label\n",
      "  - sidebar_position\n",
      "  - source\n",
      "\n",
      "Sample document content (first 200 chars):\n",
      "Source: documents/docs/people.mdx\n",
      "Content: \n",
      "\n",
      "import People from \"@theme/People\";\n",
      "\n",
      "# People\n",
      "\n",
      "There are some incredible humans from all over the world who have been instrumental in helping the LangChain community flourish üåê!\n",
      "\n",
      "This page highlight...\n"
     ]
    }
   ],
   "source": [
    "# Analyze document metadata\n",
    "file_types = {}\n",
    "metadata_keys = set()\n",
    "\n",
    "for doc in documents:\n",
    "    file_type = doc.metadata.get('filetype', 'unknown')\n",
    "    file_types[file_type] = file_types.get(file_type, 0) + 1\n",
    "    metadata_keys.update(doc.metadata.keys())\n",
    "\n",
    "print(\"Document types:\")\n",
    "for file_type, count in file_types.items():\n",
    "    print(f\"  - {file_type}: {count}\")\n",
    "\n",
    "print(\"\\nMetadata keys found:\")\n",
    "for key in sorted(metadata_keys):\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "# Sample document content\n",
    "if documents:\n",
    "    print(\"\\nSample document content (first 200 chars):\")\n",
    "    print(f\"Source: {documents[0].metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f45dae",
   "metadata": {},
   "source": [
    "## Split Documents into Chunks\n",
    "\n",
    "Split the documents into smaller chunks for better retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79e78b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1514 documents into 12791 chunks\n",
      "\n",
      "Sample chunk:\n",
      "Source: documents/docs/people.mdx\n",
      "Content length: 906 chars\n",
      "Content preview: import People from \"@theme/People\";\n",
      "\n",
      "# People\n",
      "\n",
      "There are some incredible humans from all over the world who have been instrumental in helping the Lang...\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "\n",
    "# Sample chunk\n",
    "if chunks:\n",
    "    print(\"\\nSample chunk:\")\n",
    "    print(f\"Source: {chunks[0].metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content length: {len(chunks[0].page_content)} chars\")\n",
    "    print(f\"Content preview: {chunks[0].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c636ae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to upload: 12791\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to use all chunks (for production)\n",
    "upload_chunks = chunks\n",
    "# For testing with a small sample, use:\n",
    "# upload_chunks = chunks[:3]\n",
    "\n",
    "print(f\"Total chunks to upload: {len(upload_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba0f1c",
   "metadata": {},
   "source": [
    "## Store Documents in Vector Database\n",
    "\n",
    "Store the document chunks in Supabase vector store for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ec69e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = SupabaseVectorStore(\n",
    "    client=supabase,\n",
    "    table_name=\"documents\",\n",
    "    query_name=\"rag_query\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dc2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing documents in vector database...\n",
      "Total chunks to upload: 12791\n",
      "Processing in 427 batches of 30 chunks each\n",
      "\n",
      "Processing batch 1/427 with 30 chunks (chunks 1-30)...\n",
      "Duplicate detected in batch 1, skipping: duplicate key value violates unique constraint \"documents_content_key\"\n",
      "\n",
      "Processing batch 2/427 with 30 chunks (chunks 31-60)...\n",
      "Duplicate detected in batch 2, skipping: duplicate key value violates unique constraint \"documents_content_key\"\n",
      "\n",
      "Processing batch 3/427 with 30 chunks (chunks 61-90)...\n",
      "Duplicate detected in batch 3, skipping: duplicate key value violates unique constraint \"documents_content_key\"\n",
      "\n",
      "Processing batch 4/427 with 30 chunks (chunks 91-120)...\n",
      "Duplicate detected in batch 4, skipping: duplicate key value violates unique constraint \"documents_content_key\"\n",
      "\n",
      "Processing batch 5/427 with 30 chunks (chunks 121-150)...\n",
      "Duplicate detected in batch 5, skipping: duplicate key value violates unique constraint \"documents_content_key\"\n",
      "\n",
      "Processing batch 6/427 with 30 chunks (chunks 151-180)...\n",
      "Non-duplicate error in batch 6: [SSL: SSLV3_ALERT_BAD_RECORD_MAC] ssl/tls alert bad record mac (_ssl.c:2580)\n",
      "\n",
      "Breaking process due to non-duplicate error.\n",
      "\n",
      "Upload process summary:\n",
      "- 0 batches successfully stored\n",
      "- 5 batches skipped due to duplicates\n",
      "- 1 batches failed with other errors\n",
      "You can now use the vector store for retrieving documents!\n"
     ]
    }
   ],
   "source": [
    "# Store documents in vector database using smaller batches with better error handling\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"Storing documents in vector database...\")\n",
    "print(f\"Total chunks to upload: {len(upload_chunks)}\")\n",
    "\n",
    "# Process in batches of 300 chunks (as requested)\n",
    "batch_size = 30\n",
    "num_chunks = len(upload_chunks)\n",
    "num_batches = math.ceil(num_chunks / batch_size)\n",
    "\n",
    "print(f\"Processing in {num_batches} batches of {batch_size} chunks each\")\n",
    "\n",
    "successful_batches = 0\n",
    "skipped_batches = 0\n",
    "failed_batches = 0\n",
    "ssl_error_batches = []  # Track batches with SSL errors\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, num_chunks)\n",
    "    current_batch = upload_chunks[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\nProcessing batch {i+1}/{num_batches} with {len(current_batch)} chunks (chunks {start_idx+1}-{end_idx})...\")\n",
    "    \n",
    "    try:       \n",
    "        # Store the current batch in the vector database\n",
    "        batch_vector_store = SupabaseVectorStore.from_documents(\n",
    "            current_batch,\n",
    "            embeddings,\n",
    "            client=supabase,\n",
    "            table_name=\"documents\",\n",
    "            query_name=\"rag_query\",\n",
    "            chunk_size=30  # Smaller internal chunk size for API calls\n",
    "        )\n",
    "        \n",
    "        \n",
    "        print(f\"Batch {i+1}/{num_batches} successfully stored!\")\n",
    "        successful_batches += 1\n",
    "        \n",
    "        # Add a small delay between batches to avoid rate limits\n",
    "        if i < num_batches - 1:\n",
    "            print(\"Waiting 1 second before processing next batch...\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_message = str(e) if not hasattr(e, 'message') else str(e.message)\n",
    "        \n",
    "        # Check if it's an SSL error\n",
    "        if any(ssl_pattern in error_message.lower() for ssl_pattern in [\"ssl\", \"tls\", \"certificate\", \"handshake\", \"bad record\"]):\n",
    "            ssl_error_batches.append({\n",
    "                \"batch_number\": i+1,\n",
    "                \"batch_size\": len(current_batch),\n",
    "                \"start_idx\": start_idx,\n",
    "                \"end_idx\": end_idx,\n",
    "                \"error\": error_message\n",
    "            })\n",
    "            failed_batches += 1\n",
    "            print(f\"SSL Error in batch {i+1}: {error_message}\")\n",
    "            print(\"\\nRecorded SSL error information. Trying to continue with next batch...\")\n",
    "            # Add a longer delay after SSL error before trying next batch\n",
    "            time.sleep(3)\n",
    "            continue\n",
    "            \n",
    "        # Check if it's a duplicate exception (look for common duplicate error patterns)\n",
    "        if any(dup_pattern in error_message.lower() for dup_pattern in [\"duplicate\", \"already exists\", \"unique constraint\", \"unique violation\", \"conflict\"]):\n",
    "            skipped_batches += 1\n",
    "            print(f\"Duplicate detected in batch {i+1}, skipping: {error_message}\")\n",
    "            \n",
    "            # Add a small delay before continuing\n",
    "            time.sleep(0.5)\n",
    "            continue  # Skip this batch and continue with the next one\n",
    "        else:\n",
    "            # For any other exception, break the loop\n",
    "            failed_batches += 1\n",
    "            print(f\"Non-duplicate error in batch {i+1}: {error_message}\")\n",
    "            print(\"\\nBreaking process due to non-duplicate error.\")\n",
    "            break  # Exit the loop on any non-duplicate error\n",
    "\n",
    "print(f\"\\nUpload process summary:\")\n",
    "print(f\"- {successful_batches} batches successfully stored\")\n",
    "print(f\"- {skipped_batches} batches skipped due to duplicates\")\n",
    "print(f\"- {failed_batches} batches failed with other errors\")\n",
    "\n",
    "# Display SSL error information if any were recorded\n",
    "if ssl_error_batches:\n",
    "    print(\"\\nSSL Error Information:\")\n",
    "    print(f\"Total SSL errors: {len(ssl_error_batches)}\")\n",
    "    for i, error_info in enumerate(ssl_error_batches):\n",
    "        print(f\"\\nSSL Error #{i+1}:\")\n",
    "        print(f\"  Batch number: {error_info['batch_number']}\")\n",
    "        print(f\"  Batch size: {error_info['batch_size']}\")\n",
    "        print(f\"  Chunk range: {error_info['start_idx']+1}-{error_info['end_idx']}\")\n",
    "        print(f\"  Error: {error_info['error']}\")\n",
    "\n",
    "if successful_batches > 0 or skipped_batches > 0:\n",
    "    print(\"You can now use the vector store for retrieving documents!\")\n",
    "    \n",
    "    # Create a vector store instance for retrieval if needed\n",
    "    vector_store = SupabaseVectorStore(\n",
    "        client=supabase,\n",
    "        table_name=\"documents\",\n",
    "        query_name=\"rag_query\",\n",
    "        embedding=embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a042feb",
   "metadata": {},
   "source": [
    "## Test Retrieval\n",
    "\n",
    "Test the retrieval functionality with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2765cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Test with a sample query\n",
    "query = \"How do agents work in LangChain?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"Retrieved {len(docs)} documents for query: '{query}'\")\n",
    "print(\"\\nRetrieved documents:\")\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"File type: {doc.metadata.get('filetype', 'Unknown')}\")\n",
    "    print(f\"Content (first 200 chars): {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d145c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
